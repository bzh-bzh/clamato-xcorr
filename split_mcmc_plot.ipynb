{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddb492-df5d-4aa4-a12d-dfe243496431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import collections\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from astropy.io import ascii\n",
    "from vega import VegaInterface\n",
    "import emcee\n",
    "import corner\n",
    "import yaml\n",
    "import scipy.optimize, scipy.stats\n",
    "%pylab inline\n",
    "\n",
    "import constants\n",
    "from MCMC_Run import one_dimensional_log_lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba480f6-a358-40b5-8b59-a70d1323a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING = {\n",
    "    'bias_QSO': r'$b_{gal}$', \n",
    "    'beta_QSO': r'$\\beta_{gal}$', \n",
    "    'sigma_velo_disp_gauss_QSO': r'$\\sigma_z$',\n",
    "    'drp_QSO': r'$\\delta_z$',\n",
    "    'bias_hcd': r'$b_{DLA}$',\n",
    "    'beta_hcd': r'$\\beta_{DLA}$',\n",
    "    'L0_hcd': r'$L_{DLA}$'\n",
    "}\n",
    "\n",
    "SURVEY_TITLE_MAPPING = {\n",
    "    '3dhst': '3D-HST',\n",
    "    'clamato': 'CLAMATO',\n",
    "    'mosdef': 'MOSDEF',\n",
    "    'vuds': 'VUDS',\n",
    "    'zDeep': 'COSMOS-zDEEP'\n",
    "}\n",
    "\n",
    "STACKED_MASS_BIN_TITLE_MAPPING = {\n",
    "    'lowmass': 'LM',\n",
    "    'medmass': 'MM',\n",
    "    'highmass': 'HM'\n",
    "}\n",
    "\n",
    "constants.MCMC_DIR_BASE = '/global/homes/b/bzh/clamato-xcorr/data/split-mcmc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3949e54-4513-44ef-a06c-61d154b9e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_w_data(survey_name, param_dict, model_num_params,\n",
    "                      SigMin = 0., SigMax = 30.,\n",
    "                      PiMin = -30., PiMax = 30.,\n",
    "                      one_dimensional=False):\n",
    "\n",
    "    PiBins0 = ascii.read(Path(constants.XCORR_DIR_BASE) / 'bins23_pi_0-30hMpc.txt')\n",
    "    SigBins0 = ascii.read(Path(constants.XCORR_DIR_BASE) / 'bins10_sigma_0-30hMpc.txt')\n",
    "    cosmo = constants.COSMOLOGY\n",
    "    \n",
    "    XCorr_obs = np.load(Path(constants.XCORR_DIR_BASE) / 'split' / f'xcorr_{survey_name}_globalf_{constants.DATA_VERSION}.npy')\n",
    "\n",
    "    PiEdges = PiBins0['pi_edges'].data\n",
    "    SigEdges = SigBins0['sigma_edges'].data\n",
    "\n",
    "    SigEdgesVec, PiEdgesVec = np.meshgrid(SigEdges, PiEdges)\n",
    "\n",
    "    SigEdgesPlot = SigEdgesVec / (np.ones(np.shape(SigEdgesVec))*[cosmo.h])\n",
    "    PiEdgesPlot = PiEdgesVec / (np.ones(np.shape(PiEdgesVec))*[cosmo.h])\n",
    "    \n",
    "    SigCentersPlot = (SigEdges[1:] + SigEdges[:-1]) / 2\n",
    "    SigCentersPlot /= cosmo.h\n",
    "\n",
    "    vega = VegaInterface(Path(constants.MCMC_DIR_BASE) / survey_name.split('_')[0] / f'main_{survey_name}.ini')\n",
    "    if one_dimensional:\n",
    "        VegaInterface.log_lik = one_dimensional_log_lik\n",
    "    \n",
    "    for k, v in param_dict.items():\n",
    "        vega.params[k] = v\n",
    "    if 'beta_QSO' not in param_dict and 'bias_eta_QSO' not in param_dict:\n",
    "        vega.params['beta_QSO'] = vega.params['growth_rate'] / vega.params['bias_QSO']\n",
    "    XModel = vega.compute_model()['qsoxlya'].reshape(*XCorr_obs.shape)\n",
    "    XModelPlot = XModel.T\n",
    "        \n",
    "    # Verify that the model is now in the same shape as the observed (which should be in the \n",
    "    # 'original' shape)\n",
    "    XCorrPlot = np.transpose(XCorr_obs)\n",
    "    if not np.array_equal(np.shape(XModelPlot),np.shape(XCorrPlot)):\n",
    "        if np.array_equal(np.shape(np.transpose(XCorrPlot)),np.shape(XModelPlot)):\n",
    "            XCorrPlot = np.transpose(XCorrPlot)\n",
    "        else:\n",
    "            print('Input XCorr array not compatible with model array!')\n",
    "            \n",
    "    if one_dimensional:\n",
    "        _, (chi2, reduced_covar, mask_nonflat) = vega.log_lik(return_aux=True)\n",
    "\n",
    "        # Reduce xcorrs.\n",
    "        XCorrPlot = np.ma.array(XCorrPlot, mask=mask_nonflat.T).sum(axis=0)\n",
    "        XModelPlot = np.ma.array(XModelPlot, mask=mask_nonflat.T).sum(axis=0)\n",
    "        assert len(XCorrPlot) == len(SigCentersPlot)\n",
    "        assert XCorrPlot.ndim == 1\n",
    "\n",
    "        plt.errorbar(SigCentersPlot, XCorrPlot, yerr=np.sqrt(np.diag(reduced_covar)), color='black', label='Observed')\n",
    "        plt.plot(SigCentersPlot, XModelPlot, color='red', label='Best-fit model')\n",
    "        plt.xlabel(r'$\\sigma\\; (\\mathrm{cMpc})$')\n",
    "        plt.ylabel(r'$\\pi$-summed cross-correlation')\n",
    "        plt.title(f'Reduced $\\chi^2$ = {chi2 / (np.sum(~XCorrPlot.mask) - model_num_params):.3f}', y=0.9, fontsize=10)\n",
    "    else:\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2,figsize=(5,5))\n",
    "\n",
    "        ax1.pcolormesh(SigEdgesPlot, PiEdgesPlot, XCorrPlot,cmap='jet_r',vmin=-0.2, vmax=0.1 )\n",
    "        ax1.set_aspect('equal')\n",
    "        ax1.set_xlim(0., SigMax)\n",
    "        ax1.set_ylim(PiMin, PiMax)\n",
    "        ax1.set_xlabel(r'$\\sigma\\; (\\mathrm{cMpc})$')\n",
    "        ax1.set_ylabel(r'$\\pi\\; (\\mathrm{cMpc})$')\n",
    "        # ax1.set_title(survey_name,fontsize=10)\n",
    "\n",
    "        ax2.pcolormesh(SigEdgesPlot, PiEdgesPlot, XModelPlot,cmap='jet_r',vmin=-0.2, vmax=0.1 )\n",
    "        ax2.set_aspect('equal')\n",
    "        ax2.set_xlim(SigMin, SigMax)\n",
    "        ax2.set_ylim(PiMin, PiMax)\n",
    "        ax2.set_xlabel(r'$\\sigma\\; (\\mathrm{cMpc})$')\n",
    "        # ax2.set_title(f'chi-sq = {vega.chi2():.1f}',fontsize=10)\n",
    "        # print(vega.corr_items['qsoxlya'].rp_rt_grid[1][~vega.data['qsoxlya'].mask])\n",
    "        fig.suptitle(f'Reduced $\\chi^2$ = {vega.chi2() / (np.sum(vega.data[\"qsoxlya\"].mask) - model_num_params):.3f}', y=0.9, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c292e6-22d2-476d-a540-ecf5742ba7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corner_plot(survey_name, chain_suffix='', config_suffix=None, show_grid_fit=False, one_dimensional=False, **kwargs):\n",
    "    backend = emcee.backends.HDFBackend(os.path.join(constants.MCMC_DIR_BASE, survey_name, f'chain_{survey_name}{chain_suffix}.hdf5'), read_only=True)\n",
    "    config = None\n",
    "    if config_suffix is None:\n",
    "        config_suffix = chain_suffix\n",
    "    for p in glob.glob('mcmc_cfg/*.yaml'):\n",
    "        if p.split('/')[-1].casefold() == f'{survey_name.casefold()}_split{config_suffix}.yaml':\n",
    "            with open(p, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "    if not config:\n",
    "        raise RuntimeError\n",
    "    if 'beta_QSO' in config['initial'] and config['initial']['beta_QSO'] is None:\n",
    "        del config['initial']['beta_QSO']\n",
    "    tau = backend.get_autocorr_time(quiet=True)\n",
    "    burnin = int(2 * np.max(tau))\n",
    "    thin = int(0.5 * np.min(tau))\n",
    "    # thin = int(np.max(tau))\n",
    "    chain_len = len(backend.get_chain())\n",
    "    samples = backend.get_chain(discard=burnin, flat=True, thin=thin)\n",
    "    \n",
    "    print(f\"Autocorrelation time: {tau}\")\n",
    "    print(f\"Chain length: {chain_len}; Chain length / 50: {chain_len / 50:.2f}\")\n",
    "    print(\"Burn-in: {0}\".format(burnin))\n",
    "    print(\"Thin: {0}\".format(thin))\n",
    "    print(\"Flat chain shape: {0}; {1:.1f} autocorrelation times.\".format(samples.shape, samples.shape[0] / np.max(tau)))\n",
    "    \n",
    "    final_medians = np.median(samples, axis=0)\n",
    "    unflat_samples = backend.get_chain(discard=burnin, flat=False, thin=thin)\n",
    "    # print(unflat_samples.shape)\n",
    "    for i in range(1, unflat_samples.shape[0]):\n",
    "        subsample = unflat_samples[:i].reshape(-1, samples.shape[-1])\n",
    "        if np.allclose(np.median(subsample, axis=0), final_medians, rtol=0.05):\n",
    "            print(f'# chain iterations to arrive at ~5% parameter estimate: {i * thin + burnin}')\n",
    "            break\n",
    "    \n",
    "    if show_grid_fit:\n",
    "        truths = config['initial'].values()\n",
    "    else:\n",
    "        truths = None\n",
    "        \n",
    "    # For corner plot, multiply delta_z by -1 so we're consistent with definitions.\n",
    "    delta_z_inverted_samples = np.copy(samples)\n",
    "    try:\n",
    "        assert 'drp_QSO' in config['initial']\n",
    "        delta_z_ind = -1\n",
    "        delta_z_inverted_samples[:, delta_z_ind] *= -1\n",
    "        if truths:\n",
    "            truths[delta_z_ind] *= -1\n",
    "    except AssertionError:\n",
    "        pass\n",
    "    \n",
    "    # Divide dispersion by 2 to get to what we consider z-dispersion.\n",
    "    disp_corrected_samples = np.copy(delta_z_inverted_samples)\n",
    "    try:\n",
    "        assert 'sigma_velo_disp_gauss_QSO' in config['initial']\n",
    "        sigma_z_ind = -2\n",
    "        disp_corrected_samples[:, sigma_z_ind] /= 2\n",
    "        if truths:\n",
    "            truths[sigma_z_ind] /= 2\n",
    "    except AssertionError:\n",
    "        pass\n",
    "    \n",
    "    corner_labels = []\n",
    "    for mass_bin in constants.STACKED_BIN_TITLES:\n",
    "        corner_labels.append(f\"{STACKED_MASS_BIN_TITLE_MAPPING[mass_bin]} {LABEL_MAPPING['bias_QSO']}\")\n",
    "    if 'sigma_velo_disp_gauss_QSO' in config['initial']:\n",
    "        corner_labels.append(LABEL_MAPPING['sigma_velo_disp_gauss_QSO'])\n",
    "    if 'drp_QSO' in config['initial']:\n",
    "        corner_labels.append(LABEL_MAPPING['drp_QSO'])\n",
    "    \n",
    "    corner.corner(disp_corrected_samples, labels=corner_labels, show_titles=True, \n",
    "                  truths=truths, truth_color='red',\n",
    "                  **kwargs)\n",
    "\n",
    "    if one_dimensional:\n",
    "        bias_last_ind = None\n",
    "    else:\n",
    "        bias_last_ind = -2\n",
    "    bias_corrcoef = np.corrcoef(samples[:,  :bias_last_ind], rowvar=False)\n",
    "    plt.text(0.5, 0.8, f'Bias correlation matrix:\\n{bias_corrcoef}', transform=plt.gcf().transFigure)\n",
    "    \n",
    "    plt.suptitle(SURVEY_TITLE_MAPPING[survey_name], x=0.54, weight='bold', fontsize=14)\n",
    "    plt.savefig(os.path.join(constants.FIG_DIR_BASE, 'split-mcmc', f'titledcorner_{survey_name}{chain_suffix}.png'))\n",
    "    plt.suptitle('')\n",
    "    plt.savefig(os.path.join(constants.FIG_DIR_BASE, 'split-mcmc', f'corner_{survey_name}{chain_suffix}.pdf'))\n",
    "    plt.savefig(os.path.join(constants.FIG_DIR_BASE, 'split-mcmc', f'corner_{survey_name}{chain_suffix}.png'))\n",
    "    plt.show()\n",
    "    for i, mass_bin in enumerate(constants.STACKED_BIN_TITLES):\n",
    "        median_params = {\n",
    "            'bias_QSO': np.median(samples[:, i])\n",
    "        }\n",
    "        if 'sigma_velo_disp_gauss_QSO' in config['initial']:\n",
    "            median_params['sigma_velo_disp_gauss_QSO'] = np.median(samples[:, sigma_z_ind])\n",
    "        if 'drp_QSO' in config['initial']:\n",
    "            median_params['drp_QSO'] = np.median(samples[:, delta_z_ind])\n",
    "            \n",
    "        if 'fixed' in config and config['fixed']:\n",
    "            for k, p in config['fixed'].items():\n",
    "                median_params[k] = p\n",
    "        plot_model_w_data(f'{survey_name}_{mass_bin}', median_params, samples.shape[1] - (len(constants.STACKED_BIN_TITLES) - 1), one_dimensional=one_dimensional)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "        plt.savefig(os.path.join(constants.FIG_DIR_BASE, 'split-mcmc', f'bestmodel_{survey_name}_{mass_bin}{chain_suffix}.pdf'))\n",
    "        plt.savefig(os.path.join(constants.FIG_DIR_BASE, 'split-mcmc', f'bestmodel_{survey_name}_{mass_bin}{chain_suffix}.png'))\n",
    "        if one_dimensional:\n",
    "            plt.title(f'{STACKED_MASS_BIN_TITLE_MAPPING[mass_bin]} {SURVEY_TITLE_MAPPING[survey_name]} | ' + plt.gca().get_title(), y=0.9, fontsize=10)\n",
    "        else:\n",
    "            plt.suptitle(f'{STACKED_MASS_BIN_TITLE_MAPPING[mass_bin]} {SURVEY_TITLE_MAPPING[survey_name]} | ' + plt.gcf()._suptitle.get_text(), y=0.9, fontsize=10)\n",
    "        plt.savefig(os.path.join(constants.FIG_DIR_BASE, 'split-mcmc', f'titledbestmodel_{survey_name}_{mass_bin}{chain_suffix}.png'))\n",
    "        plt.show()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c719cd-30e7-4ddb-a618-859db6f21176",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2d = {}\n",
    "sample_1d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430189b-d9f0-4a4d-b19a-4e5eecb71dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_2d['clamato'] = corner_plot('clamato', chain_suffix='', config_suffix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9699e-443b-46ec-88e2-829c0b91c083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_1d['clamato'] = corner_plot('clamato', chain_suffix='_oned', one_dimensional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad91459-4734-4657-b2d0-fdc9355cda12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2d['vuds'] = corner_plot('vuds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd6838-b06f-43f7-b4ea-8fb8bae03497",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1d['vuds'] = corner_plot('vuds', chain_suffix='_oned', one_dimensional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5697cb-acd3-4578-92f7-ca1eb613fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2d['zDeep'] = corner_plot('zDeep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f60e1e-51e0-4328-bb90-fbd33dab6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1d['zDeep'] = corner_plot('zDeep', chain_suffix='_oned', one_dimensional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc623f-acf6-4a09-af27-8627d5a1f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2d['mosdef'] = corner_plot('mosdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ae280-0ed4-44f9-ac4f-e3abd56422d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1d['mosdef'] = corner_plot('mosdef', chain_suffix='_oned', one_dimensional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47000e-3981-4f3c-9152-317a2f90ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1.5, 5))\n",
    "img = plt.imshow(np.array([[-0.2, 0.1]]), cmap='jet_r')\n",
    "img.set_visible(False)\n",
    "plt.axis('off')\n",
    "plt.colorbar(fraction=1)\n",
    "plt.savefig(os.path.join(constants.FIG_DIR_BASE, 'split-mcmc', 'bestmodel_colorbar.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06dc474-bf53-4f14-a05d-115d682dd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd()\n",
    "os.chdir(os.path.join(constants.FIG_DIR_BASE, 'split-mcmc'))\n",
    "os.system('./create-mosaic.sh')\n",
    "os.chdir(pwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951522b-6e7b-4497-997e-d8f8a2f4b231",
   "metadata": {},
   "source": [
    "# Marginalized bias distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f34c89-95f0-4eea-b456-094f0b99a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_mode(data, n_bins):\n",
    "    '''Estimates mode of data from a histogram\n",
    "\n",
    "    Args:\n",
    "        data: Samples.\n",
    "        n_bins: Number of histogram bins to use.\n",
    "\n",
    "    Returns:\n",
    "        Mode estimated from histogram.\n",
    "    '''\n",
    "    hist, edges = np.histogram(data, n_bins)\n",
    "    # Center the bin edges.\n",
    "    edges += (edges[1] - edges[0]) / 2\n",
    "    return edges[np.argmax(hist)]\n",
    "\n",
    "def variance_from_mode(data, n_bins):\n",
    "    '''Compute variance, but using histogram-estimated mode as central value,\n",
    "    instead of mean.\n",
    "\n",
    "    Args:\n",
    "        data: Samples.\n",
    "        n_bins: Number of histogram bins to use for estimating the mode.\n",
    "\n",
    "    Returns:\n",
    "        \"Variance\" using mode as central value.\n",
    "    '''\n",
    "    mode = hist_mode(data, n_bins)\n",
    "    return np.sum((data - mode)**2) / (len(data) - 1)\n",
    "\n",
    "def fit_truncated_normal(data):\n",
    "    '''Fit a truncated normal distribution to data using maximum-likelihood estimation,\n",
    "    via LFBGS-B.\n",
    "\n",
    "    Args:\n",
    "        data: Samples.\n",
    "\n",
    "    Returns:\n",
    "        fit_params: Best-fit parameters of truncated normal model; mean and standard deviation.\n",
    "        Boolean on whether optimizer converged or not.\n",
    "    '''\n",
    "    if data.ndim > 1:\n",
    "        data = data.flatten()\n",
    "    mode = hist_mode(data, 20)\n",
    "    def target_log_prob_fn(theta):\n",
    "        loc, scale = theta\n",
    "        # Support is in terms of z-values for whatever reason (thanks, scipy).\n",
    "        # Fix truncation to [0, inf).\n",
    "        a, b = (0 - loc) / scale, np.inf\n",
    "        return -np.sum(scipy.stats.truncnorm.logpdf(data, a, b, loc=loc, scale=scale))\n",
    "    # Optimizer fails spectacularly sometimes (fits 0 mean, extremely large sigma), so try to avoid this by bounding.\n",
    "    mean_bounds = (0, np.max(data))\n",
    "    sigma_bounds = (np.std(data), np.max(data) - np.min(data))\n",
    "    result = scipy.optimize.minimize(target_log_prob_fn, \n",
    "                                     (mode, variance_from_mode(data, 20)**0.5),\n",
    "                                     bounds=((mean_bounds[0], mean_bounds[1]), (sigma_bounds[0], sigma_bounds[1])))\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e8dea-14fb-485a-893d-184112efa75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "sample_2d_fits = {k: [] for k in sample_2d.keys()}\n",
    "\n",
    "def plot_normal(axis, x, loc, scale, color):\n",
    "    a, b = (0 - loc) / scale, (np.inf - loc) / scale\n",
    "    axis.plot(x, scipy.stats.truncnorm.pdf(x, a, b, loc=loc, scale=scale), color=color, alpha=1)\n",
    "    \n",
    "def get_1sigma_bounds(loc, scale, percentile=(16, 50, 84)):\n",
    "    a, b = (0 - loc) / scale, (np.inf - loc) / scale\n",
    "    truncnorm = scipy.stats.truncnorm(a, b, loc=loc, scale=scale)\n",
    "    return truncnorm.ppf(np.array(percentile) / 100)\n",
    "\n",
    "for survey, samples in sample_2d.items():\n",
    "    for i, label in enumerate(['Low mass', 'Medium mass', 'High mass']):\n",
    "        _, _, patches = axes[i].hist(samples[:, i], bins=100, alpha=0.4, label=survey, density=True)\n",
    "        axes[i].set_xlim(0, None)\n",
    "        patchcolor = patches.patches[0].get_facecolor()\n",
    "        mean, sigma = fit_truncated_normal(samples[:, i])\n",
    "        plot_normal(axes[i], np.linspace(*axes[i].get_xlim(), 1000), mean, sigma, patchcolor)\n",
    "        axes[i].set_title(label)\n",
    "        sample_2d_fits[survey].append((mean, sigma))\n",
    "plt.suptitle('2D cross-correlation')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0cf37-d01a-45f7-8078-c73b34ca60b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "sample_1d_fits = {k: [] for k in sample_2d.keys()}\n",
    "\n",
    "for survey, samples in sample_1d.items():\n",
    "    for i, label in enumerate(['Low mass', 'Medium mass', 'High mass']):\n",
    "        _, _, patches = axes[i].hist(samples[:, i], bins=100, alpha=0.4, label=survey, density=True)\n",
    "        axes[i].set_xlim(0, None)\n",
    "        patchcolor = patches.patches[0].get_facecolor()\n",
    "        mean, sigma = fit_truncated_normal(samples[:, i])\n",
    "        plot_normal(axes[i], np.linspace(*axes[i].get_xlim(), 1000), mean, sigma, patchcolor)\n",
    "        axes[i].set_title(label)\n",
    "        sample_1d_fits[survey].append((mean, sigma))\n",
    "plt.suptitle('1D cross-correlation')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a83802-2ed7-40e2-987f-fe030577a3d8",
   "metadata": {},
   "source": [
    "## Combine PDFs (naive multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e287349-7451-4035-a9de-c3574ce9dfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_hist_range = (0, 10)\n",
    "global_hist_nbins = 100\n",
    "global_hist_width = (global_hist_range[1] - global_hist_range[0]) / global_hist_nbins\n",
    "\n",
    "global_hist_bin_edges = np.histogram([], bins=global_hist_nbins, range=global_hist_range)[1]\n",
    "\n",
    "def compute_multiplied_histogram(samples_list, ind):\n",
    "    # Has shape (# surveys, # histogram bins).\n",
    "    hist_list = []\n",
    "    for samples in samples_list:\n",
    "        param = samples[:, ind]\n",
    "        hist_list.append(np.histogram(param, bins=global_hist_nbins, range=global_hist_range, density=True)[0])\n",
    "    # Reduce by multiplying along the first (survey) axis.\n",
    "    combined_hist = np.prod(np.array(hist_list), axis=0)\n",
    "    assert len(combined_hist) == global_hist_nbins\n",
    "    # Renormalize the histogram.\n",
    "    combined_hist /= np.sum(combined_hist * global_hist_width)\n",
    "    assert np.isclose(np.sum(combined_hist) * global_hist_width, 1)\n",
    "    return combined_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c4fd2-5239-4bcf-ad6a-8880f7c0ade0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://math.stackexchange.com/questions/114420/calculate-the-product-of-two-gaussian-pdfs\n",
    "def compute_multiplied_gaussian_pdfs(theta_1, theta_2):\n",
    "    mu_1, sig_1 = theta_1\n",
    "    mu_2, sig_2 = theta_2\n",
    "    sig = (sig_1**-2 + sig_2**-2)**-0.5\n",
    "    mu = (mu_1 * sig_2**2 + mu_2 * sig_1**2) / (sig_1**2 + sig_2**2)\n",
    "    return mu, sig\n",
    "\n",
    "# # https://stackoverflow.com/questions/5558418/list-of-dicts-to-from-dict-of-lists\n",
    "def dictoflist_to_listofdict(dol):\n",
    "    return [dict(zip(dol,t)) for t in zip(*dol.values())]\n",
    "\n",
    "combined_norm_params_2d = []\n",
    "\n",
    "for param_survey_dict in dictoflist_to_listofdict(sample_2d_fits):\n",
    "    param_list = param_survey_dict.values()\n",
    "    combined_theta = None\n",
    "    for p in param_list:\n",
    "        if combined_theta is None:\n",
    "            combined_theta = p\n",
    "        else:\n",
    "            combined_theta = compute_multiplied_gaussian_pdfs(combined_theta, p)\n",
    "    combined_norm_params_2d.append(combined_theta)\n",
    "    \n",
    "combined_norm_params_1d = []\n",
    "\n",
    "for param_survey_dict in dictoflist_to_listofdict(sample_1d_fits):\n",
    "    param_list = param_survey_dict.values()\n",
    "    combined_theta = None\n",
    "    for p in param_list:\n",
    "        if combined_theta is None:\n",
    "            combined_theta = p\n",
    "        else:\n",
    "            combined_theta = compute_multiplied_gaussian_pdfs(combined_theta, p)\n",
    "    combined_norm_params_1d.append(combined_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5911d-6eac-4e86-929f-0bfbcd30968e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(combined_norm_params_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0126d5-b59d-4248-8823-ff987d360809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, sharex=True, figsize=(5, 8))\n",
    "\n",
    "bias_onesig_percentiles_2d_hist = []\n",
    "bias_onesig_percentiles_1d_hist = []\n",
    "\n",
    "bias_onesig_percentiles_2d = []\n",
    "bias_onesig_percentiles_1d = []\n",
    "\n",
    "def binned_percentile(h, percentile):\n",
    "    cs = np.cumsum(h)\n",
    "    bin_idx = np.argwhere(cs >= percentile / 100 * np.sum(h))[0][0]\n",
    "    return global_hist_bin_edges[bin_idx] + (global_hist_width / 2)\n",
    "\n",
    "for i, (label, color) in enumerate(zip(['Low mass', 'Medium mass', 'High mass'], ['green', 'orange', 'red'])):\n",
    "    plt.sca(axes[i])\n",
    "    plt.xlim(*global_hist_range)\n",
    "\n",
    "    mult_hist_2d = compute_multiplied_histogram(sample_2d.values(), i)\n",
    "    mult_hist_1d = compute_multiplied_histogram(sample_1d.values(), i)\n",
    "    \n",
    "    patch = plt.stairs(mult_hist_2d, global_hist_bin_edges, label=f'2D {label}', fill=True, color=color, alpha=0.5)\n",
    "    patchcolor = patch.get_facecolor()\n",
    "    plot_normal(plt.gca(), np.linspace(*plt.gca().get_xlim(), 1000), *combined_norm_params_2d[i], patchcolor)\n",
    "    \n",
    "    plt.stairs(mult_hist_1d, global_hist_bin_edges, label=f'1D {label}',\n",
    "               color='black', hatch='/')\n",
    "    plot_normal(plt.gca(), np.linspace(*plt.gca().get_xlim(), 1000), *combined_norm_params_1d[i], 'black')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.ylabel('PDF')\n",
    "    \n",
    "    bias_onesig_percentiles_2d_hist.append((binned_percentile(mult_hist_2d, 16), binned_percentile(mult_hist_2d, 50), binned_percentile(mult_hist_2d, 84)))\n",
    "    bias_onesig_percentiles_1d_hist.append((binned_percentile(mult_hist_1d, 16), binned_percentile(mult_hist_1d, 50), binned_percentile(mult_hist_1d, 84)))\n",
    "    # Use multiplied continuous RVs instead.\n",
    "    bias_onesig_percentiles_2d.append(get_1sigma_bounds(*combined_norm_params_2d[i]))\n",
    "    bias_onesig_percentiles_1d.append(get_1sigma_bounds(*combined_norm_params_1d[i]))\n",
    "    \n",
    "plt.xlabel(r'$b_{gal}$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce9653-2e59-4cc9-8f42-5ee3e4a9457a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(bias_onesig_percentiles_2d_hist)\n",
    "print(bias_onesig_percentiles_1d_hist)\n",
    "\n",
    "print('----------')\n",
    "\n",
    "print(bias_onesig_percentiles_2d)\n",
    "print(bias_onesig_percentiles_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbeff79-c62d-411d-9444-5e5bfe899e58",
   "metadata": {},
   "source": [
    "## Overplot really rough halo mass-stellar mass estimates onto LATIS figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b661a2-ed1d-4623-a6c3-51ac0ab7051a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(constants.BIAS_DIR_BASE, 'theoretical', 'sim_bias_binwidth_0.1_binstep_0.1.pickle'), 'rb') as f:\n",
    "    theoretical_mass, theoretical_bias = pickle.load(f)\n",
    "    \n",
    "hmass_bias_theor_interp_func = lambda b: np.interp(b, theoretical_bias, theoretical_mass)\n",
    "\n",
    "hmass_onesig_percentiles_theor_2d = hmass_bias_theor_interp_func(np.array(bias_onesig_percentiles_2d))\n",
    "hmass_onesig_percentiles_theor_1d = hmass_bias_theor_interp_func(np.array(bias_onesig_percentiles_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b6d28-ea8a-4ed8-aa04-cf34782489b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(constants.BIAS_DIR_BASE, 'vega', f'hmass_bias_CLAMATO.pickle'), 'rb') as f:\n",
    "    vega_mass, vega_bias = pickle.load(f)\n",
    "    \n",
    "hmass_bias_vega_interp_func = lambda b: np.interp(b, vega_bias, vega_mass)\n",
    "\n",
    "hmass_onesig_percentiles_vega_2d = hmass_bias_vega_interp_func(np.array(bias_onesig_percentiles_2d))\n",
    "hmass_onesig_percentiles_vega_1d = hmass_bias_vega_interp_func(np.array(bias_onesig_percentiles_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26f092-b481-47ab-a264-af03dc2aae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from stacked_observations.ipynb\n",
    "bin_smass = [9.013238636363637, 9.7173801010101, 10.30939241813602]\n",
    "yerr_theor_2d = np.abs(hmass_onesig_percentiles_theor_2d[:, (0, 2)] - hmass_onesig_percentiles_theor_2d[:, 1, np.newaxis])\n",
    "yerr_theor_1d = np.abs(hmass_onesig_percentiles_theor_1d[:, (0, 2)] - hmass_onesig_percentiles_theor_1d[:, 1, np.newaxis])\n",
    "\n",
    "yerr_vega_2d = np.abs(hmass_onesig_percentiles_vega_2d[:, (0, 2)] - hmass_onesig_percentiles_vega_2d[:, 1, np.newaxis])\n",
    "yerr_vega_1d = np.abs(hmass_onesig_percentiles_vega_1d[:, (0, 2)] - hmass_onesig_percentiles_vega_1d[:, 1, np.newaxis])\n",
    "\n",
    "plt.errorbar(bin_smass, hmass_onesig_percentiles_theor_1d[:, 1], yerr=yerr_theor_1d.T, \n",
    "             lw=0, elinewidth=2, capsize=8, markeredgewidth=2, label='1D (theoretical bias curve)', color='orange', fmt='o')\n",
    "plt.errorbar(bin_smass, hmass_onesig_percentiles_theor_2d[:, 1], yerr=yerr_theor_2d.T, \n",
    "             lw=0, elinewidth=2, capsize=8, markeredgewidth=2, label='2D (theoretical bias curve)', color='red', fmt='o')\n",
    "plt.xlim(8.5, 10.7)\n",
    "plt.ylim(9, 13)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('/global/homes/b/bzh/clamato-xcorr/figures/split-mcmc/hmass-smass-theoretical.png', transparent=True, dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bdc77f-d448-4237-840c-9e9e94b85186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.errorbar(bin_smass, hmass_onesig_percentiles_vega_1d[:, 1], yerr=yerr_vega_1d.T, \n",
    "             lw=0, elinewidth=2, capsize=8, markeredgewidth=2, label='1D (Vega-fitted bias curve)', color='purple', fmt='o')\n",
    "plt.errorbar(bin_smass, hmass_onesig_percentiles_vega_2d[:, 1], yerr=yerr_vega_2d.T, \n",
    "             lw=0, elinewidth=2, capsize=8, markeredgewidth=2, label='2D (Vega-fitted bias curve)', color='pink', fmt='o')\n",
    "\n",
    "plt.xlim(8.5, 10.7)\n",
    "plt.ylim(9, 13)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('/global/homes/b/bzh/clamato-xcorr/figures/split-mcmc/hmass-smass-vegacurve.png', transparent=True, dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94f67a-189c-4c05-b56e-40c8c7b30d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vega",
   "language": "python",
   "name": "vega"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
